{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News summarization & Topic Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fJClNYCRQh0OQ6zORPwam8fnpLO7FNb3",
      "authorship_tag": "ABX9TyOzMy+++esj4iq1sFjs3/wX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranshurastogi29/News-summarization-Topic-prediction/blob/main/News_summarization_%26_Topic_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install colorama\n",
        "!pip install transformers\n",
        "!pip install -q streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUsEZbSPfdP2",
        "outputId": "384c8dd8-2f02-4632-b51a-9996b21c24ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.4\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n",
            "\u001b[K     |████████████████████████████████| 9.3 MB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 43.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 164 kB 69.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 69.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 111 kB 56.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 55.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 792 kB 46.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 374 kB 59.2 MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.24 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.7.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.31.1 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OSKpxi_hMkP",
        "outputId": "f9353a0c-9e94-43bd-c153-fc16c3fa0437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-21 12:03:01--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.237.133.81, 18.205.222.128, 52.202.168.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.237.133.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  15.8MB/s    in 0.8s    \n",
            "\n",
            "2022-01-21 12:03:02 (15.8 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "S0i52R58NJKC",
        "outputId": "fed89782-dd7d-409b-86e2-9f63e8057429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.10.0+cu111'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "%cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d pranshu29/plain-roberta-base-train\n",
        "!unzip plain-roberta-base-train.zip -d roberta_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvSnHNgh86rV",
        "outputId": "c14ed48e-ba67-4c29-ba35-e06d9c823220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Downloading plain-roberta-base-train.zip to /content\n",
            " 96% 280M/291M [00:02<00:00, 134MB/s]\n",
            "100% 291M/291M [00:02<00:00, 127MB/s]\n",
            "Archive:  plain-roberta-base-train.zip\n",
            "  inflating: roberta_train/last-checkpoint.bin  \n",
            "  inflating: roberta_train/out.csv   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle \n",
        "\n",
        "class ToxicSimpleNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, path):\n",
        "        super(ToxicSimpleNNModel, self).__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(path)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.linear = nn.Linear(in_features=self.backbone.pooler.dense.out_features*2,out_features=8)\n",
        "        \n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        seq_x, _= self.backbone(input_ids=input_ids, attention_mask=attention_masks, return_dict=False)\n",
        "        apool = torch.mean(seq_x, 1)\n",
        "        mpool, _ = torch.max(seq_x, 1)\n",
        "        x = torch.cat((apool, mpool), 1)\n",
        "        x = self.dropout(x)\n",
        "        return self.linear(x)\n",
        "\n",
        "def load_topic_model(base_path, model_path):\n",
        "  net = ToxicSimpleNNModel(base_path)\n",
        "  net.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "  return net\n",
        "\n",
        "@st.cache(allow_output_mutation=True,suppress_st_warning=True)\n",
        "def Topic_generation_load(base_path, model_path, tokenizer_path):\n",
        "    print('loading topic_model')\n",
        "    with open(tokenizer_path , 'rb') as f: \n",
        "      tokenizer = pickle.load(f)\n",
        "    model = load_topic_model(base_path, model_path)\n",
        "    return model , tokenizer\n",
        "\n",
        "@st.cache(allow_output_mutation=True,suppress_st_warning=True)\n",
        "def load_summarization_model():\n",
        "    print('loading summarization model')\n",
        "    summarization_pipe = pipeline('summarization')\n",
        "    print('sentiment model loading')\n",
        "    return summarization_pipe\n",
        "\n",
        "def get_summarization(text, summarization, max_lenght):\n",
        "  return summarization(text, max_length=max_lenght)[0]['summary_text']\n",
        "\n",
        "\n",
        "def predict_topic(text, tokenizer):\n",
        "  encoded = tokenizer.encode_plus(\n",
        "            text, \n",
        "            add_special_tokens=True, \n",
        "            max_length=512, \n",
        "            pad_to_max_length=True\n",
        "        )\n",
        "  tokens = torch.tensor(encoded['input_ids']).unsqueeze(0)\n",
        "  attention_masks = torch.tensor(encoded['attention_mask']).unsqueeze(0)\n",
        "  outputs = model(tokens, attention_masks)\n",
        "  topics = nn.functional.sigmoid(outputs).detach().numpy()\n",
        "  _ , topics = torch.topk(torch.tensor(topics), dim = 1, k = 3)\n",
        "  topics = np.array(topics)\n",
        "  top_dic = {'0':'business','1':'elections','2':'entertainment',\n",
        "             '3':'news','4':'opinion','5':'sci-tech','6':'society',\n",
        "             '7':'sport'}\n",
        "  l = []\n",
        "  for i in topics[0]:\n",
        "    l.append(top_dic[str(i)])\n",
        "  return l\n",
        "\n",
        "summarization_pipe = load_summarization_model()\n",
        "model , tokenizer = Topic_generation_load('tiny-bert' ,'model.bin', 'tokenizer.obj')\n",
        "\n",
        "st.title('News Summary Generation and Topic Prediction')\n",
        "\n",
        "st.markdown('Here you can enter the News in first text box and can get news summary around the subject')\n",
        "\n",
        "text = st.text_input('Enter News here:',key=0)\n",
        "\n",
        "if st.checkbox('Start Generate Summary'):\n",
        "    st.write('uncheck the box if you are done')\n",
        "    option = st.sidebar.selectbox(label='Max_Lenght',options=['20','40','50'])\n",
        "    summary = get_summarization(text, summarization_pipe, int(option))\n",
        "    st.write('Final summary ' + summary)\n",
        "else: pass\n",
        "\n",
        "st.markdown('Once done you can get the top Topics the news relate too')\n",
        "\n",
        "if st.button('Predict Topics'):\n",
        "  l = predict_topic(text, tokenizer)\n",
        "  st.markdown('Top topics related are '+' , '.join(l))\n",
        "else: pass\n"
      ],
      "metadata": {
        "id": "253rGR7EhVFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c81a7e88-03eb-4df4-9ebc-c25d95ee47fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./ngrok authtoken 21yJFX87jVygzsrMEb2rePp0j1G_K3UkVeGWRtiBDRmuGFRJ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p69NL7jxBQv8",
        "outputId": "464ab3a0-b2be-495f-eb4f-554ced2806fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 8501 &')"
      ],
      "metadata": {
        "id": "SVqBlD0GhfgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    'import sys, json; print(\"Execute the next cell and the go to the following URL: \" +json.load(sys.stdin)[\"tunnels\"][0][\"public_url\"])'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YfCIBfq-ob1",
        "outputId": "27eb3ac0-8968-4aa1-bd4c-dab65e8259f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execute the next cell and the go to the following URL: http://814d-34-83-152-139.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run \"app.py\""
      ],
      "metadata": {
        "id": "wxPZZH12hjZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8e511a-3cc1-467f-d25d-831c1ea1789e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.83.152.139:8501\u001b[0m\n",
            "\u001b[0m\n",
            "loading summarization model\n",
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
            "sentiment model loading\n",
            "loading topic_model\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/tiny-bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Wt3UMCtsJuMY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}